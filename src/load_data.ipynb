{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8cd1b7",
   "metadata": {},
   "source": [
    "# Airbnb Data Loading to Cassandra\n",
    "\n",
    "This notebook loads Airbnb data from CSV files into a Cassandra database.\n",
    "The data includes listings, neighbourhoods, calendar, and reviews for multiple cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8946e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from cassandra.cluster import Cluster\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c1efa",
   "metadata": {},
   "source": [
    "## Connect to Cassandra\n",
    "\n",
    "First, we'll establish a connection to our local Cassandra cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "707fb2ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequest",
     "evalue": "Error from server: code=2200 [Invalid query] message=\"Index listings_host_id_idx_1 is a duplicate of existing index listings_host_id_idx\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidRequest\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m stmt \u001b[38;5;129;01min\u001b[39;00m statements:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stmt.startswith(\u001b[33m'\u001b[39m\u001b[33m--\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m stmt:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/cassandra/cluster.py:2677\u001b[39m, in \u001b[36mSession.execute\u001b[39m\u001b[34m(self, query, parameters, timeout, trace, custom_payload, execution_profile, paging_state, host, execute_as)\u001b[39m\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, parameters=\u001b[38;5;28;01mNone\u001b[39;00m, timeout=_NOT_SET, trace=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2635\u001b[39m             custom_payload=\u001b[38;5;28;01mNone\u001b[39;00m, execution_profile=EXEC_PROFILE_DEFAULT,\n\u001b[32m   2636\u001b[39m             paging_state=\u001b[38;5;28;01mNone\u001b[39;00m, host=\u001b[38;5;28;01mNone\u001b[39;00m, execute_as=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2638\u001b[39m \u001b[33;03m    Execute the given query and synchronously wait for the response.\u001b[39;00m\n\u001b[32m   2639\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2674\u001b[39m \u001b[33;03m    on a DSE cluster.\u001b[39;00m\n\u001b[32m   2675\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_payload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_profile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaging_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_as\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/cassandra/cluster.py:4956\u001b[39m, in \u001b[36mResponseFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   4954\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ResultSet(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m._final_result)\n\u001b[32m   4955\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4956\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_exception\n",
      "\u001b[31mInvalidRequest\u001b[39m: Error from server: code=2200 [Invalid query] message=\"Index listings_host_id_idx_1 is a duplicate of existing index listings_host_id_idx\""
     ]
    }
   ],
   "source": [
    "cluster = Cluster(['127.0.0.1'])\n",
    "session = cluster.connect()\n",
    "schema_file_path = Path('schema.cql')\n",
    "with open(schema_file_path, 'r') as f:\n",
    "            schema_content = f.read()\n",
    "statements = [stmt.strip() for stmt in schema_content.split(';') if stmt.strip()]\n",
    "            \n",
    "for stmt in statements:\n",
    "    if not stmt.startswith('--') and stmt:\n",
    "        session.execute(stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0bbec0",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Let's define some helper functions to clean and convert data for Cassandra compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e2ebcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for data cleaning and format conversion\n",
    "\n",
    "def clean_price(price_str):\n",
    "    \"\"\"Clean price strings to numeric values\"\"\"\n",
    "    if pd.isna(price_str) or price_str == '':\n",
    "        return None\n",
    "    if isinstance(price_str, (int, float)):\n",
    "        return float(price_str)\n",
    "    # Remove currency symbols and commas\n",
    "    if isinstance(price_str, str):\n",
    "        cleaned = re.sub(r'[^\\d.]', '', price_str)\n",
    "        return float(cleaned) if cleaned else None\n",
    "    return None\n",
    "\n",
    "def convert_date(date_str):\n",
    "    \"\"\"Convert date strings to datetime objects\"\"\"\n",
    "    if pd.isna(date_str) or date_str == '':\n",
    "        return None\n",
    "    if isinstance(date_str, str):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str).date()\n",
    "        except:\n",
    "            return None\n",
    "    if isinstance(date_str, pd.Timestamp):\n",
    "        return date_str.date()\n",
    "    return None\n",
    "\n",
    "def convert_bool(value):\n",
    "    \"\"\"Convert various boolean representations to True/False\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return False\n",
    "    if isinstance(value, bool):\n",
    "        return value\n",
    "    if isinstance(value, str):\n",
    "        return value.lower() in ('true', 't', 'yes', 'y', '1')\n",
    "    if isinstance(value, (int, float)):\n",
    "        return bool(value)\n",
    "    return False\n",
    "\n",
    "def safe_int(value):\n",
    "    \"\"\"Convert values to integers safely\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return 0\n",
    "    try:\n",
    "        return int(float(value))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def safe_float(value):\n",
    "    \"\"\"Convert values to floats safely\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return 0.0\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def safe_text(value):\n",
    "    \"\"\"Convert values to strings safely for Cassandra text type\"\"\"\n",
    "    if pd.isna(value) or value == '':\n",
    "        return None\n",
    "    # Escape single quotes for Cassandra\n",
    "    if isinstance(value, str):\n",
    "        return value.replace(\"'\", \"''\")\n",
    "    return str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c53e22e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to load data into Cassandra\n",
    "\n",
    "def load_neighbourhoods(file_path):\n",
    "    \"\"\"Load neighbourhoods data into Cassandra\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading neighbourhoods from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        \n",
    "        # Check if required columns exist\n",
    "        if 'neighbourhood' not in df.columns or 'city' not in df.columns:\n",
    "            print(\"Required columns missing in neighbourhoods data\")\n",
    "            return 0\n",
    "        \n",
    "        # Ensure neighbourhood_group column exists\n",
    "        if 'neighbourhood_group' not in df.columns:\n",
    "            df['neighbourhood_group'] = None\n",
    "            \n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO my_keyspace.neighbourhoods (city, neighbourhood_group, neighbourhood)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        prepared = session.prepare(insert_query)\n",
    "        count = 0\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Inserting neighbourhoods\"):\n",
    "            city = safe_text(row['city'])\n",
    "            neighbourhood_group = safe_text(row.get('neighbourhood_group', ''))\n",
    "            neighbourhood = safe_text(row['neighbourhood'])\n",
    "            \n",
    "            if city and neighbourhood:\n",
    "                session.execute(prepared, (city, neighbourhood_group, neighbourhood))\n",
    "                count += 1\n",
    "        \n",
    "        print(f\"Inserted {count} neighbourhood records\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading neighbourhoods: {e}\")\n",
    "        return 0\n",
    "\n",
    "def load_listings(file_path):\n",
    "    \"\"\"Load listings data into Cassandra\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading listings from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows with NA in required columns\n",
    "        original_count = len(df)\n",
    "        df = df.dropna(subset=['id', 'city'])\n",
    "        dropped_count = original_count - len(df)\n",
    "        print(f\"Dropped {dropped_count} rows with NA values in required columns from listings data\")\n",
    "        \n",
    "        if 'id' not in df.columns or 'city' not in df.columns:\n",
    "            print(\"Required columns missing in listings data\")\n",
    "            return 0\n",
    "        \n",
    "        # Ensure neighbourhood column exists for primary key\n",
    "        if 'neighbourhood' not in df.columns and 'neighbourhood_cleansed' in df.columns:\n",
    "            df['neighbourhood'] = df['neighbourhood_cleansed']\n",
    "        elif 'neighbourhood' not in df.columns:\n",
    "            df['neighbourhood'] = 'Unknown'\n",
    "            \n",
    "        # Create insert query\n",
    "        columns = [\n",
    "            'id', 'name', 'description', 'host_id', 'host_name', 'host_since',\n",
    "            'host_is_superhost', 'neighbourhood', 'neighbourhood_group', 'city',\n",
    "            'latitude', 'longitude', 'property_type', 'room_type', 'accommodates',\n",
    "            'bedrooms', 'beds', 'price', 'minimum_nights', 'maximum_nights',\n",
    "            'number_of_reviews', 'review_scores_rating', 'calculated_host_listings_count'\n",
    "        ]\n",
    "        \n",
    "        # Check which columns actually exist in the DataFrame\n",
    "        existing_cols = [col for col in columns if col in df.columns]\n",
    "        \n",
    "        placeholders = ', '.join(['?'] * len(existing_cols))\n",
    "        col_str = ', '.join(existing_cols)\n",
    "        \n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO my_keyspace.listings ({col_str})\n",
    "            VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        prepared = session.prepare(insert_query)\n",
    "        count = 0\n",
    "        batch_size = 100\n",
    "        batch = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preparing listings\"):\n",
    "            try:\n",
    "                values = []\n",
    "                for col in existing_cols:\n",
    "                    value = row.get(col)\n",
    "                    if col == 'id' or col == 'host_id':\n",
    "                        values.append(safe_int(value))\n",
    "                    elif col == 'host_since':\n",
    "                        values.append(convert_date(value))\n",
    "                    elif col == 'host_is_superhost':\n",
    "                        values.append(convert_bool(value))\n",
    "                    elif col in ['price']:\n",
    "                        values.append(clean_price(value))\n",
    "                    elif col in ['latitude', 'longitude', 'review_scores_rating']:\n",
    "                        values.append(safe_float(value))\n",
    "                    elif col in ['accommodates', 'bedrooms', 'beds', 'minimum_nights',\n",
    "                                'maximum_nights', 'number_of_reviews', 'calculated_host_listings_count']:\n",
    "                        values.append(safe_int(value))\n",
    "                    else:\n",
    "                        values.append(safe_text(value))\n",
    "                        \n",
    "                batch.append(values)\n",
    "                \n",
    "                # Execute in batches to improve performance\n",
    "                if len(batch) >= batch_size:\n",
    "                    for b in batch:\n",
    "                        session.execute(prepared, b)\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "            except Exception as row_err:\n",
    "                print(f\"Error processing row {idx}: {row_err}\")\n",
    "                continue\n",
    "        \n",
    "        # Insert any remaining records\n",
    "        if batch:\n",
    "            for b in batch:\n",
    "                try:\n",
    "                    session.execute(prepared, b)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch record: {e}\")\n",
    "        \n",
    "        print(f\"Inserted {count} listing records\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading listings: {e}\")\n",
    "        return 0\n",
    "    \n",
    "def load_calendar(file_path):\n",
    "    \"\"\"Load calendar data into Cassandra\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading calendar from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows with NA in required columns\n",
    "        original_count = len(df)\n",
    "        df = df.dropna(subset=['listing_id', 'date'])\n",
    "        dropped_count = original_count - len(df)\n",
    "        print(f\"Dropped {dropped_count} rows with NA values in required columns from calendar data\")\n",
    "        \n",
    "        if 'listing_id' not in df.columns or 'date' not in df.columns:\n",
    "            print(\"Required columns missing in calendar data\")\n",
    "            return 0\n",
    "        \n",
    "        # Convert date column to datetime\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        \n",
    "        # Process price columns\n",
    "        if 'price' in df.columns:\n",
    "            df['price'] = df['price'].apply(clean_price)\n",
    "        if 'adjusted_price' in df.columns:\n",
    "            df['adjusted_price'] = df['adjusted_price'].apply(clean_price)\n",
    "            \n",
    "        # Convert available column if needed\n",
    "        if 'available' in df.columns:\n",
    "            df['available'] = df['available'].apply(convert_bool)\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO my_keyspace.calendar \n",
    "            (listing_id, date, available, price, adjusted_price, minimum_nights, maximum_nights)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        prepared = session.prepare(insert_query)\n",
    "        count = 0\n",
    "        batch_size = 500\n",
    "        batch = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Inserting calendar data\"):\n",
    "            try:\n",
    "                listing_id = safe_int(row['listing_id'])\n",
    "                date = row['date']\n",
    "                available = convert_bool(row.get('available', True))\n",
    "                price = clean_price(row.get('price', 0))\n",
    "                adjusted_price = clean_price(row.get('adjusted_price', price))\n",
    "                min_nights = safe_int(row.get('minimum_nights', 1))\n",
    "                max_nights = safe_int(row.get('maximum_nights', 365))\n",
    "                \n",
    "                batch.append((listing_id, date, available, price, adjusted_price, min_nights, max_nights))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    for b in batch:\n",
    "                        session.execute(prepared, b)\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "            except Exception as row_err:\n",
    "                print(f\"Error processing calendar row {idx}: {row_err}\")\n",
    "                continue\n",
    "        \n",
    "        # Insert any remaining records\n",
    "        if batch:\n",
    "            for b in batch:\n",
    "                try:\n",
    "                    session.execute(prepared, b)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch calendar record: {e}\")\n",
    "        \n",
    "        print(f\"Inserted {count} calendar records\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading calendar: {e}\")\n",
    "        return 0\n",
    "\n",
    "def load_reviews(file_path):\n",
    "    \"\"\"Load reviews data into Cassandra\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading reviews from: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Drop rows with NA in required columns\n",
    "        original_count = len(df)\n",
    "        df = df.dropna(subset=['listing_id', 'date', 'id'])\n",
    "        dropped_count = original_count - len(df)\n",
    "        print(f\"Dropped {dropped_count} rows with NA values in required columns from reviews data\")\n",
    "        \n",
    "        if 'listing_id' not in df.columns or 'date' not in df.columns or 'id' not in df.columns:\n",
    "            print(\"Required columns missing in reviews data\")\n",
    "            return 0\n",
    "        \n",
    "        # Convert date column\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "        \n",
    "        insert_query = \"\"\"\n",
    "            INSERT INTO my_keyspace.reviews \n",
    "            (id, listing_id, date, reviewer_id, reviewer_name, comments)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        prepared = session.prepare(insert_query)\n",
    "        count = 0\n",
    "        batch_size = 250\n",
    "        batch = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Inserting reviews\"):\n",
    "            try:\n",
    "                review_id = safe_int(row['id'])\n",
    "                listing_id = safe_int(row['listing_id'])\n",
    "                date = row['date']\n",
    "                reviewer_id = safe_int(row.get('reviewer_id', 0))\n",
    "                reviewer_name = safe_text(row.get('reviewer_name', ''))\n",
    "                comments = safe_text(row.get('comments', ''))\n",
    "                \n",
    "                batch.append((review_id, listing_id, date, reviewer_id, reviewer_name, comments))\n",
    "                \n",
    "                if len(batch) >= batch_size:\n",
    "                    for b in batch:\n",
    "                        session.execute(prepared, b)\n",
    "                    count += len(batch)\n",
    "                    batch = []\n",
    "                    \n",
    "            except Exception as row_err:\n",
    "                print(f\"Error processing review row {idx}: {row_err}\")\n",
    "                continue\n",
    "        \n",
    "        # Insert any remaining records\n",
    "        if batch:\n",
    "            for b in batch:\n",
    "                try:\n",
    "                    session.execute(prepared, b)\n",
    "                    count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error inserting batch review record: {e}\")\n",
    "        \n",
    "        print(f\"Inserted {count} review records\")\n",
    "        return count\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading reviews: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e805679",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data into Cassandra\n",
    "\n",
    "Now let's load the preprocessed data into our Cassandra database. We'll use the preprocessed_*.csv files that were created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f6cc3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All preprocessed files found. Starting data loading...\n",
      "\n",
      "Loading neighbourhoods from: ../Airbnb Data/preprocessed_neighbourhoods.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting neighbourhoods: 100%|██████████| 481/481 [00:00<00:00, 1468.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 481 neighbourhood records\n",
      "Loaded 481 neighbourhoods in 0.58 seconds\n",
      "\n",
      "Loading listings from: ../Airbnb Data/preprocessed_listings.csv\n",
      "Dropped 0 rows with NA values in required columns from listings data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing listings: 100%|██████████| 62771/62771 [00:55<00:00, 1133.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 62771 listing records\n",
      "Loaded 62771 listings in 56.39 seconds\n",
      "\n",
      "Loading calendar from: ../Airbnb Data/preprocessed_calendar.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m8/rtmqck7x7cx4ccts2v7r4kzh0000gn/T/ipykernel_96792/2037408710.py:142: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with NA values in required columns from calendar data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting calendar data: 100%|██████████| 22902530/22902530 [2:29:06<00:00, 2559.84it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 22902530 calendar records\n",
      "Loaded 22902530 calendar entries in 9019.98 seconds\n",
      "\n",
      "Loading reviews from: ../Airbnb Data/preprocessed_reviews.csv\n",
      "Dropped 0 rows with NA values in required columns from reviews data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting reviews: 100%|██████████| 3003791/3003791 [17:47<00:00, 2813.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3003791 review records\n",
      "Loaded 3003791 reviews in 1087.55 seconds\n",
      "\n",
      "\n",
      "Data loading complete!\n",
      "Total records loaded: 25969573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define base directory for the preprocessed data\n",
    "BASE_DIR = Path('../Airbnb Data')\n",
    "\n",
    "# File paths for preprocessed data\n",
    "listings_path = BASE_DIR / 'preprocessed_listings.csv'\n",
    "neighbourhoods_path = BASE_DIR / 'preprocessed_neighbourhoods.csv'\n",
    "calendar_path = BASE_DIR / 'preprocessed_calendar.csv'\n",
    "reviews_path = BASE_DIR / 'preprocessed_reviews.csv'\n",
    "\n",
    "# Verify files exist\n",
    "files_exist = True\n",
    "for file_path in [listings_path, neighbourhoods_path, calendar_path, reviews_path]:\n",
    "    if not file_path.exists():\n",
    "        print(f\"Warning: {file_path} does not exist\")\n",
    "        files_exist = False\n",
    "\n",
    "if files_exist:\n",
    "    print(\"All preprocessed files found. Starting data loading...\\n\")\n",
    "    \n",
    "    # Load neighbourhoods first (since they're referenced by listings)\n",
    "    start_time = time.time()\n",
    "    neighbourhood_count = load_neighbourhoods(neighbourhoods_path)\n",
    "    print(f\"Loaded {neighbourhood_count} neighbourhoods in {time.time() - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Load listings\n",
    "    start_time = time.time()\n",
    "    listings_count = load_listings(listings_path)\n",
    "    print(f\"Loaded {listings_count} listings in {time.time() - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Load calendar data (can be very large, so we process it last)\n",
    "    start_time = time.time()\n",
    "    calendar_count = load_calendar(calendar_path)\n",
    "    print(f\"Loaded {calendar_count} calendar entries in {time.time() - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Load reviews\n",
    "    start_time = time.time()\n",
    "    reviews_count = load_reviews(reviews_path)\n",
    "    print(f\"Loaded {reviews_count} reviews in {time.time() - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\nData loading complete!\")\n",
    "    print(f\"Total records loaded: {neighbourhood_count + listings_count + calendar_count + reviews_count}\")\n",
    "else:\n",
    "    print(\"Some preprocessed files are missing. Please run the preprocessing notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1515f585",
   "metadata": {},
   "source": [
    "## Verify Data in Cassandra\n",
    "\n",
    "Let's run some queries to verify that our data loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reviews from: ../Airbnb Data/preprocessed_reviews.csv\n",
      "Dropped 0 rows with NA values in required columns from reviews data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting reviews: 100%|██████████| 3003791/3003791 [17:37<00:00, 2841.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3003791 review records\n",
      "Loaded 3003791 reviews in 1074.36 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "reviews_count = load_reviews(reviews_path)\n",
    "print(f\"Loaded {reviews_count} reviews in {time.time() - start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c95a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error counting calendar: errors={'127.0.0.1:9042': 'Client request timeout. See Session.execute[_async](timeout)'}, last_host=127.0.0.1:9042\n",
      "Error counting reviews: Error from server: code=1300 [Replica(s) failed to execute read] message=\"Operation failed - received 0 responses and 1 failures: UNKNOWN from localhost/127.0.0.1:7000\" info={'consistency': 'LOCAL_ONE', 'required_responses': 1, 'received_responses': 0, 'failures': 1, 'error_code_map': {'127.0.0.1': '0x0000'}}\n",
      "Records in Cassandra database:\n",
      "Neighbourhoods: 481\n",
      "Listings: 62771\n",
      "Calendar entries: 0\n",
      "Reviews: 0\n",
      "\n",
      "Sample data from each table:\n",
      "\n",
      "Sample neighbourhoods:\n",
      "Row(city='Salem', neighbourhood='Ward 1', neighbourhood_group=None)\n",
      "Row(city='Salem', neighbourhood='Ward 2', neighbourhood_group=None)\n",
      "Row(city='Salem', neighbourhood='Ward 3', neighbourhood_group=None)\n",
      "Row(city='Salem', neighbourhood='Ward 4', neighbourhood_group=None)\n",
      "Row(city='Salem', neighbourhood='Ward 5', neighbourhood_group=None)\n",
      "\n",
      "Sample listings:\n",
      "Row(id=43181760, name='Oceanfront- Modern Beach Home on The Strand !', city='Los Angeles', neighbourhood='Hermosa Beach', room_type='Entire home/apt')\n",
      "Row(id=1110359772813668352, name='Brynhurst Private Bedroom 6 II', city='Los Angeles', neighbourhood='Hyde Park', room_type='Private room')\n",
      "Row(id=8691474, name='Koreatown Nook near Staples Center', city='Los Angeles', neighbourhood='Pico-Union', room_type='Entire home/apt')\n",
      "Row(id=905823713038962560, name='Moments Away From Concert Venue', city='San Diego', neighbourhood='Oak Park', room_type='Private room')\n",
      "Row(id=798535, name='Detached Silver Lake Studio - Middle of it all!', city='Los Angeles', neighbourhood='Silver Lake', room_type='Entire home/apt')\n",
      "\n",
      "Sample calendar entries:\n",
      "Row(listing_id=43181760, date=Date(20150), available=False, price=Decimal('242'))\n",
      "Row(listing_id=43181760, date=Date(20151), available=False, price=Decimal('242'))\n",
      "Row(listing_id=43181760, date=Date(20152), available=False, price=Decimal('242'))\n",
      "Row(listing_id=43181760, date=Date(20153), available=False, price=Decimal('242'))\n",
      "Row(listing_id=43181760, date=Date(20154), available=False, price=Decimal('242'))\n",
      "\n",
      "Sample reviews:\n",
      "Row(id=1324801543443282944, listing_id=43181760, date=Date(20089), reviewer_name='Elias')\n",
      "Row(id=1309413349876512256, listing_id=43181760, date=Date(20068), reviewer_name='Kenneth')\n",
      "Row(id=1287216993067836416, listing_id=43181760, date=Date(20037), reviewer_name='Oliver')\n",
      "Row(id=1280591212816907520, listing_id=43181760, date=Date(20028), reviewer_name='Sheri')\n",
      "Row(id=1235654772665217280, listing_id=43181760, date=Date(19966), reviewer_name='Mahmoudnouh@Google.Com')\n"
     ]
    }
   ],
   "source": [
    "# Verify data in each table\n",
    "def count_records(table_name):\n",
    "    query = f\"SELECT COUNT(*) FROM my_keyspace.{table_name}\"\n",
    "    try:\n",
    "        result = session.execute(query)\n",
    "        return result.one()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting {table_name}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Get counts for each table\n",
    "neighbourhoods_count = count_records('neighbourhoods')\n",
    "listings_count = count_records('listings')\n",
    "calendar_count = count_records('calendar')\n",
    "reviews_count = count_records('reviews')\n",
    "\n",
    "print(\"Records in Cassandra database:\")\n",
    "print(f\"Neighbourhoods: {neighbourhoods_count}\")\n",
    "print(f\"Listings: {listings_count}\")\n",
    "print(f\"Calendar entries: {calendar_count}\")\n",
    "print(f\"Reviews: {reviews_count}\")\n",
    "\n",
    "# Sample queries to verify data quality\n",
    "print(\"\\nSample data from each table:\")\n",
    "\n",
    "# Sample neighbourhoods\n",
    "print(\"\\nSample neighbourhoods:\")\n",
    "rows = session.execute(\"SELECT * FROM my_keyspace.neighbourhoods LIMIT 5\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Sample listings\n",
    "print(\"\\nSample listings:\")\n",
    "rows = session.execute(\"SELECT id, name, city, neighbourhood, room_type FROM my_keyspace.listings LIMIT 5\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Sample calendar entries\n",
    "print(\"\\nSample calendar entries:\")\n",
    "rows = session.execute(\"SELECT listing_id, date, available, price FROM my_keyspace.calendar LIMIT 5\")\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Sample reviews\n",
    "print(\"\\nSample reviews:\")\n",
    "rows = session.execute(\"SELECT id, listing_id, date, reviewer_name FROM my_keyspace.reviews LIMIT 5\")\n",
    "for row in rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be58af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Cassandra connection when done\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e39cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b43f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28ae01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
